{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting an article as a text file from a webpage\n",
    "\n",
    "For this purpose, we will use BeatifulSoup library. It is Python library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner.\n",
    "We will provide the URL of the webpage and output its content into a text file titled 'article.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "  \n",
    "# Pass url of the webpage and the path where you want to save it as a text file)\n",
    "urllib.request.urlretrieve(\"https://en.wikipedia.org/wiki/The_Hunger_Games\",\n",
    "                           \"webpage.txt\")\n",
    "\n",
    "# Open the file in reading mode  \n",
    "file = open(\"webpage.txt\", \"r\", encoding=\"UTF-8\")\n",
    "\n",
    "# Read the file\n",
    "contents = file.read()\n",
    "\n",
    "# Parse it through an HTML parser provided by BeautifulSOup library\n",
    "soup = BeautifulSoup(contents, 'html.parser')\n",
    "  \n",
    "# Open a new empty file\n",
    "f = open(\"article.txt\", \"w\", encoding=\"UTF-8\")\n",
    "  \n",
    "# traverse paragraphs from soup and save in the empty file\n",
    "text=\"\"\n",
    "for data in soup.find_all(\"p\"):\n",
    "    text = text+data.get_text()\n",
    "    f.writelines(text)\n",
    "  \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Summarization\n",
    "\n",
    "We will be using the Transformers library for text summarization. It provides general-purpose architectures for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with a number of pretrained models in many languages.\n",
    "\n",
    "Install the required libraries if not already installed using the command : pip install transformers torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Open the article in read mode\n",
    "f = open(\"article.txt\", \"r\", encoding=\"utf8\")\n",
    "to_tokenize = f.read()\n",
    "to_tokenize = to_tokenize[:1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project, we are using HuggingFace module which has performed finetuning with the CNN/DailyMail summarization dataset, providing us with pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DistilBART: \n",
    "\n",
    "DistilBART is same as BART but smaller. BART (Bidirectional and Auto-Regressive Transformers) combines a bidirectional BERT-like (Bidirectional Encoder Representations from Transformers) encoder with a GPT-like decoder (Generative Pre-Training), allowing us to benefit from BERT bidirectionality while being able to generate text, which is not one of BERTâ€™s key benefits. Using the BART architecture, we can finetune a model for summarization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HuggingFace summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarized = summarizer(to_tokenize, min_length=75, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' The Hunger Games is a series of young adult dystopian novels written by American author Suzanne Collins . The first three novels are titled \"The Hunger Games\" and \"Catching Fire\" Each was adapted for film, with the film adaptation of Mocking Games split into two feature-length films . The series is set in the Hunger Games universe, and the first two novels were both New York Times best sellers .'}]\n"
     ]
    }
   ],
   "source": [
    "# Print summarized text\n",
    "print(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store summarized text in a file\n",
    "summaryBART = open(\"summaryBART.txt\", \"w\")\n",
    "summaryBART.write(str(summarized)) # converted the list to string\n",
    "summaryBART.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using T5:\n",
    "T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format. T5 converts all NLP problems into a text-to-text format. It is trained using teacher forcing, we always need an input sequence and a corresponding target sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> the series is set in the Hunger Games universe. the first three novels are a trilogy following teenage protagonist Katniss Everdeen. each was adapted for film, establishing the film series.</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
    "inputs = tokenizer(\"summarize: \" + to_tokenize, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=200, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryT5 = open(\"summaryT5.txt\", \"w\")\n",
    "summaryT5.write(tokenizer.decode(outputs[0]))\n",
    "summaryT5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We used BeautifulSoup to retrieve textual data from a URL and then used HuggingFace to summarize that text succintly. We used both the distilled BART and T5 models.\n",
    "Our BART model seems to have provided a better summary of the article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
